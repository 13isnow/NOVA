publish time: 2024-07-15T01:23:54.000Z  
https://www.yuque.com/rhg37y/ngp5pr/sueqqg2o77xqgzxk  
author: 阮一海  
---
[距离计算](https://nova.yuque.com/rhg37y/ngp5pr/md8p1i8xtk12bd0g)

[机器学习---常见的距离公式（欧氏距离、曼哈顿距离、标准化欧式距离、余弦距离、杰卡德距离、马氏距离、切比雪夫距离、闵可夫斯基距离、K-L散度）-CSDN博客](https://blog.csdn.net/weixin_43961909/article/details/132388832)

文档写晚了，好像建模方面大家探索了不少

跟着柏均的思路水水文档吧

---

## 概率建模
好吧，让我来换一种建模方式，看看能不能玩出新花样

**叠甲：我现在连概率论都还没学，而且我估计这个思路其实殊途同归大差不差，所以就当我在口嗨，想换一种角度看待问题，解释问题就可以了**

****

+ 问卷的每一个问题对应一个随机变量$ X_i $
+ 每个问题的回答是随机变量的一个值，比如第三题选第四个选项，$ X_3 = 4 $

      （PS：这种建模方法要求所有问题均为单选题，这一点好处理）

+ 然后我们得到一组随机变量向量$ V_k=[X_1,X_2,\dots,X_n] $
+ 数据预处理
+ 权重$ W_i = [\omega_1, \omega_2, \dots, \omega_n] $
+ 通过设计，我们计算任意两位同学组成一个寝室的概率$ p=P(V_{k_i},V_{k_j}, W_{k_i}, W_{k_j}) $

<details class="lake-collapse"><summary id="u7d15a7dd"><span class="ne-text" style="color: rgb(25, 27, 31)">这里我给没有概率论基础的同学大致解释一下（虽然我也不熟）</span></summary><p id="u7e0180ad" class="ne-p"><span class="ne-text" style="color: rgb(25, 27, 31)">这里的随机变量</span><span id="Fjx4J" class="ne-math" style="color: rgb(25, 27, 31)"><img src="https://cdn.nlark.com/yuque/__latex/7ca2ff984e95d4a9e44fe7498e281020.svg"></span><span class="ne-text" style="color: rgb(25, 27, 31)">是针对所有填写问卷的同学的</span></p><p id="u27aa4c5d" class="ne-p"><span class="ne-text" style="color: rgb(25, 27, 31)">比如100人中，有30人填第1项，20人填第2项，50人填第3项</span></p><p id="u2cc051ac" class="ne-p"><span class="ne-text" style="color: rgb(25, 27, 31)">则</span><span id="opOC9" class="ne-math" style="color: rgb(25, 27, 31)"><img src="https://cdn.nlark.com/yuque/__latex/7ca2ff984e95d4a9e44fe7498e281020.svg"></span><span class="ne-text" style="color: rgb(25, 27, 31)">可取1，2，3，且</span></p><p id="u154f6794" class="ne-p"><span id="bvpWk" class="ne-math" style="color: rgb(25, 27, 31)"><img src="https://cdn.nlark.com/yuque/__latex/6b0c0d9397421dee347cccadf5a922b2.svg"></span></p><p id="u766871f7" class="ne-p"><span class="ne-text"></span></p></details>
**问题：**

1. **相似度函数**$ P $**怎么求？**
2. **数据预处理成什么样？**
3. **权重怎么设计？**

需求：

1. 适合的同学之间的概率应该更大

> 过多小差异和一个大差异可能计算出相近的距离，这种情况下就可能导致主要矛盾的突出尖锐
>

2. 对于超出均值的数据应该敏感
3. 对于绝对性问题（如是否抽烟），需要尽可能分开不同选择的人
    1. 引申：有哪些“绝对性问题（不能被容忍的问题）”？

---

### 选项赋分
一般来说，数据需要标准化消除量纲的影响

我们的赋分都是自己设计的，也许可以不用特别在意标准化

但设计的时候还是要求一下量纲一致吧

$ X_i = \displaystyle \frac{n}{N} * 10 $，n是选项标号，比如第一项n=1，N是本题选项总数目，比如5量表题N=5，$ \frac{n}{N} $是一个0到1之间的数字，乘10感觉好看一点

### 非线性放大
<font style="color:rgb(6, 6, 7);">为了避免小差异和一个大差异导致相近的距离计算，我们可以放大小差异与大差异的差距，从而减小造成计算结果相近的可能性</font>

<font style="color:rgb(6, 6, 7);">所以我们可以在距离计算中引入一个非线性项</font>

<font style="color:rgb(6, 6, 7);">比如</font>$ X_i = e^{X_i} $

至少可以在一定程度上减少<font style="color:rgb(6, 6, 7);">计算结果相近的发生</font>

<font style="color:rgb(6, 6, 7);">或者我们可以设计</font>

$ X_i = (a_i)^{X_i} $<font style="color:rgb(6, 6, 7);">,</font>$ a_i=1+\omega_i,\;\omega_i $<font style="color:rgb(6, 6, 7);">是这道题的权重，</font>$ a_i $<font style="color:rgb(6, 6, 7);">默认大于1进行一定比例的放大</font>

### <font style="color:rgb(6, 6, 7);">闵可夫斯基距离</font>
闵式距离将多个距离公式组合

$ \displaystyle D(V_{k_i}, V_{k_j}) = \sqrt[q]{\sum |V_{k_i}-V_{k_j}|^q} $

<font style="color:rgb(6, 6, 7);">当</font>$ q=1 $<font style="color:rgb(6, 6, 7);">时，D为曼哈顿距离</font>

<font style="color:rgb(6, 6, 7);">当</font>$ q=2 $<font style="color:rgb(6, 6, 7);">时，D为欧式距离</font>

<font style="color:rgb(6, 6, 7);">当</font>$ q=\infty $<font style="color:rgb(6, 6, 7);">时，D为切比雪夫距离</font>

<font style="color:rgb(6, 6, 7);">换句话说，q越大，距离值对差异越敏感，差异越大，数据的差越大</font>

<font style="color:rgb(6, 6, 7);">从柏均的探索我们得知，欧式距离对差异不太敏感，切比雪夫距离（只取最大值）对差异又太过敏感，所以我们能不能探索出一个q，使得D可以同时兼具欧式距离与切比雪夫距离的优点呢？</font>

**<font style="color:rgb(6, 6, 7);">新增问题：</font>**

+ **怎么确定当前参数q是否为最优？**
+ **怎么根据结果调整参数q？**

****

针对问题的猜想（不成熟）

从q=1开始计算，q逐渐增大，记录分类情况，因为缺乏标记数据，所以需要人主动根据分类情况选择合适的q值

分类合格标准：故意添加几个有大差异的数据，检测模型是否识别出来，并将数据分配到正确的类别

### 马氏距离
我们<font style="color:rgb(6, 6, 7);">假设每个随机变量</font>$ X_i $<font style="color:rgb(6, 6, 7);">服从正态分布，实际上这个假设也比较符合直觉</font>

<font style="color:rgb(6, 6, 7);">那么向量</font>$ V_i $<font style="color:rgb(6, 6, 7);">可以被视为多元正态分布的样本</font>

<font style="color:rgb(6, 6, 7);">马氏距离考虑到不同维度的尺度和相关性，引入了协方差矩阵参与计算</font>

$ \displaystyle D(V_{k_i}, V_{k_j}) = \sqrt{(V_{k_i}-V_{k_j})^T\; \Sigma^{-1} \;(V_{k_i}-V_{k_j})}, \Sigma = Cov(V_{k_i}, V_{k_j}) $

<font style="color:rgb(6, 6, 7);">可能比单纯的欧式距离更合理一点</font>

<font style="color:rgb(6, 6, 7);">用闵可夫斯基距离的表示形式也可以</font>

$ \displaystyle D(V_{k_i}, V_{k_j}) = \sqrt[q]{(V_{k_i}-V_{k_j})^T\; \Sigma^{-\frac{1}{q}} \;(V_{k_i}-V_{k_j})}, \Sigma = Cov(V_{k_i}, V_{k_j}),q>1 $

### <font style="color:rgb(6, 6, 7);">集成学习与投票表决</font>
我从随机森林算法里了解到一个概念[随机森林算法梳理（Random Forest）](https://zhuanlan.zhihu.com/p/57965634)

> **<font style="color:rgb(25, 27, 31);">集成学习：</font>**<font style="color:rgb(25, 27, 31);">对于训练数据集，我们通过训练一系列个体学习器，并通过一定的结合策略将它们组合起来，形成一个强有力的学习器，以达到博采众长的目的，正如“三个臭皮匠，顶个诸葛亮”</font>
>

现在大家各有各的建模，每个建模都对应一种算法，思路都很不错，最后如果要应用不一定要只选一个，我们可以根据分类情况投票表决

![画板](https://cdn.nlark.com/yuque/0/2024/jpeg/38709574/1720846569923-876fc3fe-3865-47df-a578-17e93d9b7bf7.jpeg)

投票表决的思路有很多

最简单的就是<font style="color:rgb(25, 27, 31);">简单投票法，</font>哪种分类出现次数多就选哪种

还有加权投票法，用一组已标记数据测试个体学习机，凭准确率对分类结果进行加权赋分投票

### 联合概率密度
找AI问了写概率论的知识点

如果假设每一个随机变量符合正态分布

令$ \Sigma=Cov(X_1, X_2, \dots, X_n), U = V_i.mean() $<font style="color:rgb(6, 6, 7);">均值向量</font>

$ f(V_i) = \displaystyle \frac{1}{(2\pi)^{\frac{n}{2}}\; |\Sigma|^{\frac{1}{2}}} {\exp}(-\frac{1}{2}(V_i - U)^T \Sigma (V_i - U)） $为联合概率密度函数

由此两学生相似度取$ p = P(V_{k_i},V_{k_j}) = \displaystyle \frac{f(V_{k_i}) f(V_{k_j})}{f(U)} $

> <font style="color:rgb(6, 6, 7);">这个相似度度量了两个学生向量在概率分布中的相对位置。如果两个学生向量的概率密度值都很高，那么它们被认为是相似的；如果一个或两个值都很低，那么它们被认为是不相似的</font>
>

（看不懂，感觉很高级）

等等，好像不能保证全是正态分布😥



















