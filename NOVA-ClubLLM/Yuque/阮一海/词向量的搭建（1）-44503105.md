publish time: 2023-12-16T01:35:47.000Z  
https://www.yuque.com/rhg37y/fvdi39/amaia5nu3n4ql7f3  
author: é˜®ä¸€æµ·  
---
æœç´¢ã€word2vecã€‘

[[NLP] ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨](https://zhuanlan.zhihu.com/p/26306795)

ğŸ“„[word2vec--TensorFlow](https://www.tensorflow.org/text/tutorials/word2vec#skip-gram_and_negative_sampling)

ä¾æ®ä¸Šé¢è¿™ç¯‡æ–‡ç« 

```python
f1 = open('out_demo2.txt', encoding='utf-8')#è¿™é‡Œçš„out_demo2æ˜¯åˆ†å¥½è¯çš„æ–‡ä»¶
for line in f1:
    tokens = list(line.lower().split())
    print(len(tokens), end='
')
    vocab, index = {}, 1  # start indexing from 1
    vocab['<pad>'] = 0  # add a padding token
    for token in tokens:
        if token not in vocab:
            vocab[token] = index
        index += 1
        vocab_size = len(vocab)
        print(vocab)
        inverse_vocab = {index: token for token, index in vocab.items()}
        print(inverse_vocab)
        example_sequence = [vocab[word] for word in tokens]
        print(example_sequence)

```

<details class="lake-collapse"><summary id="u1a2f2943"><span class="ne-text" style="font-size: 16px">è¿è¡Œç»“æœ1</span></summary><p id="uf36852f1" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2023/png/38709574/1702168751020-a47aa809-bee7-4ad9-a1c1-3889e26f9ac1.png" width="351" id="u071ce24e" class="ne-image"></p></details>
å¯ä»¥çœ‹åˆ°ç¼–å·æ˜¯æœ‰äº†ï¼Œä½†æ˜¯é€è¯ç¼–æ’ï¼Œæ˜¾ç„¶ä¸ç¬¦åˆåº”ç”¨åœºæ™¯

ï¼ˆæ²¡æœ‰åŸºç¡€çš„äººå°±æ˜¯è¿™æ ·ï¼Œè¿™ä¹ˆç®€å•çš„ä»£ç ä¹Ÿè¦æ…¢æ…¢ç ”ç©¶ï¼‰

ç ”ç©¶äº†ä¸€æ®µæ—¶é—´pythonä»£ç ï¼Œä¸ä¼šæï¼Œå†å»æœæœçœ‹æœ‰æ²¡æœ‰ç°æˆçš„ä»£ç è®©æˆ‘æ¬è¿

è¯´åˆ°ä»£ç ï¼Œæˆ‘å†³å®šå»githubä¸Šæ‰¾å¼€æº

æœç´¢ã€åŒ¹é…ç´¢å¼• Language:Pythonã€‘

[ElasticSearch_for_Match/code/main.py at master Â· JovenChu/ElasticSearch_for_Match](https://github.com/JovenChu/ElasticSearch_for_Match/blob/master/code/main.py)

æ¥½ï¼Œåº”è¯¥ç”¨è‹±è¯­çš„ï¼Œæœç´¢ã€index matching Language:Pythonã€‘

ç„¶è€Œï¼Œgithubä¸Šå…¨æ˜¯é¡¹ç›®ä¹‹ç±»çš„å¤§å‹ä»£ç ï¼Œè®©ä¸€ä¸ªæ–°æ‰‹å»é‡Œé¢æ‰¾è¦ç”¨åˆ°çš„ä»£ç è¿˜æ˜¯å¤ªè‹›åˆ»äº†

ä½†æ˜¯ç»è¿‡ç¼“æ…¢çš„æŸ¥æ‰¾ï¼Œæˆ‘è¿˜æ˜¯ç ”ç©¶å‡ºäº†ä¸€ç‚¹ä¸œè¥¿

```python
f1 = open('out_demo2.txt', encoding='utf-8')
vocab, index = {}, 1  # start indexing from 1
vocab['<pad>'] = 0  # add a padding token
for line in f1:
    if line not in vocab:
        line = line.replace('
', '')
        vocab[line] = index
        index += 1
vocab_size = len(vocab)
print(vocab)
inverse_vocab = {index: line for line, index in vocab.items()}
print(inverse_vocab)
f1.close()
```

<details class="lake-collapse"><summary id="u95318f54"><span class="ne-text" style="font-size: 16px">è¿è¡Œç»“æœ2</span></summary><p id="ud02da464" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2023/png/38709574/1702175175590-22607003-155d-4c52-b681-7e58fb55d5b1.png" width="1608" id="u7b787eba" class="ne-image" style="font-size: 16px"></p></details>
ç°åœ¨æœŸæœ›çš„æ˜¯å¯ä»¥æ„å»ºè¯å‘é‡ï¼Œä»¥ä¾¿åç»­åŒ¹é…

æˆ‘ä¹Ÿè®¸å¯ä»¥è®©AIå®ç°ä¸€ä¸‹

<details class="lake-collapse"><summary id="u14c02405"><span class="ne-text" style="font-size: 16px">AIå¯¹è¯</span></summary><h1 id="fLh8B"><span class="ne-text">USERï¼š</span></h1><p id="u6c5f81f7" class="ne-p"><span class="ne-text">è¯·å¸®æˆ‘å®Œå–„è¿™ä»½ä»£ç </span></p><pre data-language="python" id="BsAz8" class="ne-codeblock language-python"><code>f1 = open('out_demo2.txt', encoding='utf-8')
vocab, index = {}, 1  # start indexing from 1
vocab['&lt;pad&gt;'] = 0  # add a padding token
for line in f1:
    if line not in vocab:
        line = line.replace('
', '')
        vocab[line] = index
        index += 1
vocab_size = len(vocab)
print(vocab)
inverse_vocab = {index: line for line, index in vocab.items()}
print(inverse_vocab)
tokens = []
for line in f1:
    line = line.replace('
', '')
    print(line)
    tokens.append(line)
print(tokens)
example_sequence = [vocab[word] for word in tokens]
print(example_sequence)</code></pre><h1 id="mGAb6"><span class="ne-text">ANSWERï¼š</span></h1><pre data-language="python" id="LyhuA" class="ne-codeblock language-python"><code>f1 = open('out_demo2.txt', encoding='utf-8')
vocab, index = {}, 1  # start indexing from 1
vocab['&lt;pad&gt;'] = 0  # add a padding token

for line in f1:
    line = line.strip()  # Remove leading/trailing whitespace
    if line not in vocab:
        vocab[line] = index
        index += 1

f1.close()  # Close the file after reading

vocab_size = len(vocab)
print(vocab)

inverse_vocab = {index: line for line, index in vocab.items()}
print(inverse_vocab)

tokens = []
f1 = open('out_demo2.txt', encoding='utf-8')  # Open the file again to read from the beginning

for line in f1:
    line = line.strip()
    tokens.append(line)

f1.close()  # Close the file after reading

print(tokens)

example_sequence = [vocab[word] for word in tokens]
print(example_sequence)</code></pre><p id="u10a9da75" class="ne-p"><br></p><p id="u611ea1a4" class="ne-p"><span class="ne-text">In the code you provided, I made a few modifications to improve its functionality:</span></p><p id="u33c3b29e" class="ne-p"><br></p><ol class="ne-ol"><li id="u56da011a" data-lake-index-type="0"><span class="ne-text">Added </span><code class="ne-code"><span class="ne-text">line.strip()</span></code><span class="ne-text"> to remove leading and trailing whitespace from each line read from the file.</span></li><li id="u80475182" data-lake-index-type="0"><span class="ne-text">Closed the file after reading it using </span><code class="ne-code"><span class="ne-text">f1.close()</span></code><span class="ne-text"> to free up system resources.</span></li><li id="u4cbc76dd" data-lake-index-type="0"><span class="ne-text">Reopened the file before the second loop to read from the beginning again.</span></li><li id="u539128f6" data-lake-index-type="0"><span class="ne-text">Removed the unnecessary line </span><code class="ne-code"><span class="ne-text">line = line.replace('
', '')</span></code><span class="ne-text"> since </span><code class="ne-code"><span class="ne-text">line.strip()</span></code><span class="ne-text"> already removes newline characters.</span></li></ol><p id="ud7b41913" class="ne-p"><br></p><p id="ud9a2b982" class="ne-p"><span class="ne-text">These changes should help ensure that the code functions as intended. Let me know if you have any further questions or if there's anything else I can assist you with.</span></p><p id="u442f6022" class="ne-p"><span class="ne-text"></span></p><h1 id="myG6w"><span class="ne-text">USER:</span></h1><p id="uf71d473b" class="ne-p"><span class="ne-text">æˆ‘å¸Œæœ›å¯ä»¥éå†æ•´ç¯‡æ–‡ç« ï¼Œå¹¶é€šè¿‡vocabç»™é‡å¤å‡ºç°çš„è¯æ·»åŠ ç´¢å¼•</span></p><p id="u8cee985e" class="ne-p"><span class="ne-text"></span></p><h1 id="i2voz"><span class="ne-text">ANSWERï¼š</span></h1><pre data-language="python" id="Y2wQr" class="ne-codeblock language-python"><code>f1 = open('out_demo2.txt', encoding='utf-8')
vocab, index = {}, 1  # start indexing from 1
vocab['&lt;pad&gt;'] = 0  # add a padding token

for line in f1:
    line = line.strip()  # Remove leading/trailing whitespace
    if line not in vocab:
        vocab[line] = index
        index += 1

f1.close()  # Close the file after reading

vocab_size = len(vocab)
print(vocab)

inverse_vocab = {index: line for line, index in vocab.items()}
print(inverse_vocab)

tokens = []
f1 = open('out_demo2.txt', encoding='utf-8')  # Open the file again to read from the beginning

for line in f1:
    line = line.strip()
    tokens.append(line)

f1.close()  # Close the file after reading

print(tokens)

example_sequence = [vocab[word] for word in tokens]
print(example_sequence)</code></pre><p id="u839a30a5" class="ne-p"><br></p><p id="u19448599" class="ne-p"><span class="ne-text">è¿™æ®µä»£ç å¯ä»¥éå†æ•´ç¯‡æ–‡ç« ï¼Œå¹¶ä½¿ç”¨</span><code class="ne-code"><span class="ne-text">vocab</span></code><span class="ne-text">ä¸ºé‡å¤å‡ºç°çš„è¯æ·»åŠ ç´¢å¼•ã€‚å®ƒé¦–å…ˆæ‰“å¼€æ–‡ä»¶å¹¶åˆ›å»ºä¸€ä¸ªç©ºçš„è¯æ±‡è¡¨ï¼ˆ</span><code class="ne-code"><span class="ne-text">vocab</span></code><span class="ne-text">ï¼‰å’Œç´¢å¼•ï¼ˆ</span><code class="ne-code"><span class="ne-text">index</span></code><span class="ne-text">ï¼‰å˜é‡ã€‚ç„¶åï¼Œå®ƒé€è¡Œè¯»å–æ–‡ç« å¹¶å»é™¤æ¯è¡Œçš„é¦–å°¾ç©ºæ ¼ã€‚å¦‚æœå½“å‰è¡Œä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå®ƒä¼šå°†è¯¥è¡Œæ·»åŠ åˆ°è¯æ±‡è¡¨ï¼Œå¹¶ä¸ºå…¶åˆ†é…ä¸€ä¸ªç´¢å¼•ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå…³é—­æ–‡ä»¶ï¼Œå¹¶é‡æ–°æ‰“å¼€æ–‡ä»¶ä»¥ä»å¤´å¼€å§‹è¯»å–ã€‚ç„¶åï¼Œå®ƒå°†æ¯è¡Œæ·»åŠ åˆ°</span><code class="ne-code"><span class="ne-text">tokens</span></code><span class="ne-text">åˆ—è¡¨ä¸­ï¼Œå¹¶ä½¿ç”¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•æ›¿æ¢æ¯ä¸ªå•è¯ã€‚æœ€åï¼Œå®ƒæ‰“å°è¯æ±‡è¡¨ã€åå‘è¯æ±‡è¡¨å’Œæ›¿æ¢åçš„</span><code class="ne-code"><span class="ne-text">tokens</span></code><span class="ne-text">åˆ—è¡¨ã€‚</span></p><p id="u262d691a" class="ne-p"><br></p><p id="u498a228b" class="ne-p"><span class="ne-text">è¯·æ³¨æ„ï¼Œè¿™æ®µä»£ç å¹¶æ²¡æœ‰å¯¹æ–‡ç« è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå®ƒå°†æ•´è¡Œæ–‡æœ¬è§†ä¸ºä¸€ä¸ªå•è¯ã€‚å¦‚æœä½ å¸Œæœ›å¯¹æ–‡ç« è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œä½ å¯ä»¥ä½¿ç”¨é€‚å½“çš„åˆ†è¯å·¥å…·æˆ–åº“æ¥æ›¿æ¢ä»£ç ä¸­çš„è¡Œå¤„ç†éƒ¨åˆ†ã€‚</span></p><p id="u4a783e87" class="ne-p"><span class="ne-text"></span></p></details>
åœ¨ç»è¿‡å°å°ä¿®æ”¹

```python
#æ¥ä¸Šç«¯ä»£ç 
tokens = []
f1 = open('out_demo2.txt', encoding='utf-8')  # Open the file again to read from the beginning
for line in f1:
    line = line.strip()
    tokens.append(line)
f1.close()  # Close the file after reading
example_sequence = [vocab[word] for word in tokens]
print(example_sequence)

```

<details class="lake-collapse"><summary id="u3ec636ad"><span class="ne-text" style="font-size: 16px">è¿è¡Œç»“æœ3</span></summary><p id="ue8f11546" class="ne-p"><img src="https://cdn.nlark.com/yuque/0/2023/png/38709574/1702175973218-36bc589e-8c55-4e6d-8518-49598f45df98.png" width="1608" id="u57a7e088" class="ne-image"></p></details>
